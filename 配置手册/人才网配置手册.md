#### 以泰州就业人才网进行流程配置（代码路径：C:\Users\Administrator\PycharmProjects\公司代码）：

1. 实现方式:

   scrapy对接selenium

2. 实现效果：

   1. 数据爬取
   2. 数据存储
   3. 数据发送（钉钉群）
4. 自定义table_name，webhook_url，职位名称

##### 一、创建scrapy爬虫:

1. scrapy startproject spiderName
2. cd spiderName
3. scrapy genspider name www.xxx.com

##### 二、settings的基础设置：

1. ```python
   USER_AGENT='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'
   ```

2. ```python
   ROBOTSTXT_OBEY = False	#关闭协议
   LOG_LEVEL = 'ERROR'		#日志输出最低等级
   ```

3. ```python
   DOWNLOADER_MIDDLEWARES = {	#开启下载器中间件
      # 'TZemployment.middlewares.TzemploymentDownloaderMiddleware': 543,
      'TZemployment.middlewares.自定义中间件名字': 543,
   
   }
   ```

4. ```python
   ITEM_PIPELINES = {	#开启item
      'TZemployment.pipelines.TzemploymentPipeline': 300,
   }
   ```

##### 三、spider配置：

```python
# -*- coding: utf-8 -*-

import scrapy
from selenium import webdriver
#from .. import items
from TZtalent.items import TztalentItem
#from 主项目名.items import items中的类
class TzcodeSpider(scrapy.Spider):
    #泰州就业人才网
    name = 'tzcode'
    # allowed_domains = ['www.xxx.com']
    #不需要动，改相应的配置信息即可
    #传参
    def __init__(self,table_name,keyword,webhook,*args,**kwargs):
        super(TzcodeSpider, self).__init__(*args, **kwargs)
        # path = r"C:\Users\Administrator\Desktop\phantomjs-1.9.2-windows\phantomjs.exe"
        # self.driver = webdriver.PhantomJS(executable_path=path)
        #防止selenium识别
        options = webdriver.ChromeOptions()
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        self.driver = webdriver.Chrome(options=options)
        self.driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
            Object.defineProperty(navigator, 'webdriver', {
              get: () => undefined
            })
          """
        })
        # self.driver = webdriver.Chrome()
        ##灵活变动，有的网站统一是gb2312编码，需要将URL进行转换--urlencode编码
        #self.keyword = quote(keyword.encode("gb2312"))
        self.keyword = keyword
        self.webhook_url = webhook
        self.table_name = table_name
        self.start_urls =[f"-----------url------{self.keyword}"]
	
    #解析selenium发过来的response数据
    def parse(self, response):
        # print(response.url)
        #父标签---所需要信息标签上的父标签
        div_list = response.xpath("父标签xpath语法")
        item = TzemploymentItem()
        for div in div_list:
            item['title'] = div.xpath("./匹配的title信息xpath").extract_first()
            #判断title是否为空
            if item['title'] == None:
                break
            item['company_name'] = div.xpath("./匹配的company_name信息xpath").extract_first()
            item['company_url'] = div.xpath("./匹配的company_url信息xpath").extract_first()
            item['site'] = div.xpath('./匹配的site信息xpath').extract_first()
            yield items
    #
    def __del__(self):
        #退出驱动并关闭所有关联的窗口
        self.driver.quit()
```

##### 四、middlewares模块（不变）：

通过selenium中的page_source获取网页源码，发送至spider下的parse进行xpath解析。

```python
# -*- coding: utf-8 -*-
from scrapy import signals
from scrapy.http import HtmlResponse
import time


#设置selenium的中间件
class SelemiumSpiderMiddleware(object):
    #对发送的
    def process_request(self,request,spider):
		#通过spider调用spider下的driver属性
        spider.driver.get(request.url)
        time.sleep(1)
        page_text = spider.driver.page_source

        return HtmlResponse(url=request.url,body=page_text,request=request,encoding='utf-8')
```

##### 五、pipelines模块（不变）：

```python
# -*- coding: utf-8 -*-
import sys

sys.path.append('..')
import db
import webhook
import logging
import company

class TzemploymentPipeline(object):

    def process_item(self, item, spider):
        # webhook_url = 'https://oapi.dingtalk.com/robot/send?access_token=27d525c0827d39eb79b10ce287e02ed4b2613ddb32ad18dce07f8855e10571d4'
        # webhook_url = item["webhook_url"]
        webhook_url = spider.webhook_url
        table_name = spider.table_name
        mydb = db.MydbOperator(table_name)
        mydb.create_table()
        isInitialize = mydb.is_empty_table()
        webhook_service = webhook.WebHook(webhook_url)


        job_title = item["title"]
        company_name = item["company_name"]
        company_url = item["company_url"]
        location = item['site']
        company_in_db = mydb.get_by_company_name_and_job(company_name,job_title)
        if company_in_db is None:
            company_obj = company.company(job_title, company_name, company_url, location)
            mydb.save_company(company_obj)
            if not  isInitialize:
                # 添加 webhook发送器，不为空发送
                if not isInitialize:
                    formatted_context = webhook_service.format_with_template(company_obj)
                    print(formatted_context)
                    # 设置发送的数据格式----markdown
                    # self.webhook_service.send_markdown(company_name, formatted_context, True)
                    webhook_service.send_markdown(company_name, formatted_context, False)
        else:
            # 打印错误提示信息
            # Quit as reaching existing data records
            logging.info("没有新数据.")
```

##### 六、传参并启动爬虫：

```python
#spider中url与xpath配置完成后，进行参数设置
from scrapy import cmdline
cmdline.execute("scrapy crawl tzcode -a table_name=taizhou -a keyword=外贸业务员 -a webhook=https://oapi.dingtalk.com/robot/send?access_token=27d525c0827d39eb79b10ce287e02ed4b2613ddb32ad18dce07f8855e10571d4".split())
```

